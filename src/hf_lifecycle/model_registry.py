"""
Model registry for HuggingFace Lifecycle Manager.
"""
import torch
from pathlib import Path
from typing import Optional, Dict, Any, Union, List
from datetime import datetime
import json
import logging
import shutil
import os

from hf_lifecycle.repo import RepoManager
from hf_lifecycle.exceptions import HfLifecycleError

logger = logging.getLogger(__name__)


class ModelRegistryError(HfLifecycleError):
    """Exception for model registry operations."""
    pass


class ModelRegistry:
    """
    Registry for managing custom models on HuggingFace Hub.
    """

    def __init__(self, repo_manager: RepoManager):
        """
        Initialize the ModelRegistry.

        Args:
            repo_manager: Repository manager instance.
        """
        self.repo_manager = repo_manager

    def generate_model_card(
        self,
        model_name: str,
        description: str = "",
        architecture: Optional[str] = None,
        datasets: Optional[List[str]] = None,
        metrics: Optional[Dict[str, float]] = None,
        tags: Optional[List[str]] = None,
        license: str = "apache-2.0",
        language: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> str:
        """
        Generate a model card in Markdown format.

        Args:
            model_name: Name of the model.
            description: Model description.
            architecture: Model architecture (e.g., "bert", "gpt2").
            datasets: List of datasets used for training.
            metrics: Dictionary of evaluation metrics.
            tags: List of tags for the model.
            license: License identifier.
            language: List of languages supported.
            **kwargs: Additional metadata fields.

        Returns:
            Model card content as string.
        """
        # YAML frontmatter
        frontmatter = {
            "license": license,
            "tags": tags or ["pytorch", "custom-model"],
        }

        if language:
            frontmatter["language"] = language

        if datasets:
            frontmatter["datasets"] = datasets

        # Build card content
        card = "---\n"
        for key, value in frontmatter.items():
            if isinstance(value, list):
                card += f"{key}:\n"
                for item in value:
                    card += f"- {item}\n"
            else:
                card += f"{key}: {value}\n"
        card += "---\n\n"

        # Model header
        card += f"# {model_name}\n\n"

        # Description
        if description:
            card += f"{description}\n\n"

        # Architecture
        if architecture:
            card += f"## Architecture\n\n"
            card += f"This model uses a **{architecture}** architecture.\n\n"

        # Training details
        card += "## Training Details\n\n"
        if datasets:
            card += "### Datasets\n\n"
            for dataset in datasets:
                card += f"- {dataset}\n"
            card += "\n"

        # Metrics
        if metrics:
            card += "### Evaluation Metrics\n\n"
            card += "| Metric | Value |\n"
            card += "|--------|-------|\n"
            for metric, value in metrics.items():
                card += f"| {metric} | {value:.4f} |\n"
            card += "\n"

        # Usage example
        card += "## Usage\n\n"
        card += "```python\n"
        card += "from transformers import AutoModel, AutoTokenizer\n\n"
        card += f"model = AutoModel.from_pretrained('{{your_username}}/{model_name}')\n"
        card += f"tokenizer = AutoTokenizer.from_pretrained('{{your_username}}/{model_name}')\n"
        card += "```\n\n"

        # Additional metadata
        if kwargs:
            card += "## Additional Information\n\n"
            for key, value in kwargs.items():
                card += f"**{key.replace('_', ' ').title()}**: {value}\n\n"

        # Footer
        card += f"*Model card generated by hf_lifecycle on {datetime.now().strftime('%Y-%m-%d')}*\n"

        return card

    def register_model(
        self,
        model: torch.nn.Module,
        repo_id: str,
        model_name: Optional[str] = None,
        description: str = "",
        tokenizer: Optional[Any] = None,
        config: Optional[Any] = None,
        metrics: Optional[Dict[str, float]] = None,
        datasets: Optional[List[str]] = None,
        tags: Optional[List[str]] = None,
        private: bool = False,
        commit_message: str = "Upload model",
    ) -> str:
        """
        Register a custom model to HuggingFace Hub.

        Args:
            model: PyTorch model to register.
            repo_id: Repository ID (username/model-name).
            model_name: Display name for the model.
            description: Model description.
            tokenizer: Optional tokenizer to upload.
            config: Optional model configuration.
            metrics: Evaluation metrics.
            datasets: List of datasets used.
            tags: Model tags.
            private: Whether to create a private repository.
            commit_message: Git commit message.

        Returns:
            URL of the created repository.

        Raises:
            ModelRegistryError: If registration fails.
        """
        try:
            # Create repository
            repo_url = self.repo_manager.create_repo(
                repo_id=repo_id, repo_type="model", private=private, exist_ok=True
            )
            logger.info(f"Created/verified repository: {repo_url}")

            # Determine model name
            if model_name is None:
                model_name = repo_id.split("/")[-1]

            # Generate model card
            architecture = model.__class__.__name__
            model_card = self.generate_model_card(
                model_name=model_name,
                description=description,
                architecture=architecture,
                datasets=datasets,
                metrics=metrics,
                tags=tags,
            )

            # Upload model card
            self.repo_manager.update_card(repo_id, model_card, repo_type="model")
            logger.info("Uploaded model card")

            # Save model locally first
            local_dir = Path(f"./temp_model_{repo_id.replace('/', '_')}")
            local_dir.mkdir(parents=True, exist_ok=True)

            # Save model weights
            model_path = local_dir / "pytorch_model.bin"
            torch.save(model.state_dict(), model_path)

            # Save config if provided
            if config is not None:
                if hasattr(config, "save_pretrained"):
                    config.save_pretrained(local_dir)
                else:
                    config_path = local_dir / "config.json"
                    with open(config_path, "w") as f:
                        json.dump(config, f, indent=2)

            # Save tokenizer if provided
            if tokenizer is not None and hasattr(tokenizer, "save_pretrained"):
                tokenizer.save_pretrained(local_dir)

            # Upload files
            api = self.repo_manager._api
            for file_path in local_dir.rglob("*"):
                if file_path.is_file():
                    api.upload_file(
                        path_or_fileobj=str(file_path),
                        path_in_repo=file_path.name,
                        repo_id=repo_id,
                        repo_type="model",
                        commit_message=commit_message,
                    )

            logger.info(f"Successfully registered model to {repo_url}")

            # Cleanup
            import shutil
            shutil.rmtree(local_dir)

            return repo_url

        except Exception as e:
            raise ModelRegistryError(f"Failed to register model: {e}")

    def load_model(
        self,
        repo_id: str,
        model_class: Optional[type] = None,
        revision: Optional[str] = None,
        **kwargs: Any,
    ) -> torch.nn.Module:
        """
        Load a registered model from HuggingFace Hub.

        Args:
            repo_id: Repository ID.
            model_class: Model class to instantiate. If None, tries AutoModel.
            revision: Git revision to load from.
            **kwargs: Additional arguments for model loading.

        Returns:
            Loaded model instance.

        Raises:
            ModelRegistryError: If loading fails.
        """
        try:
            from transformers import AutoModel

            if model_class is None:
                model = AutoModel.from_pretrained(repo_id, revision=revision, **kwargs)
            else:
                model = model_class.from_pretrained(repo_id, revision=revision, **kwargs)

            logger.info(f"Loaded model from {repo_id}")
            return model

        except Exception as e:
            raise ModelRegistryError(f"Failed to load model: {e}")

    def update_model_card(
        self,
        repo_id: str,
        metrics: Optional[Dict[str, float]] = None,
        additional_info: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Update an existing model card with new metrics or information.

        Args:
            repo_id: Repository ID.
            metrics: Updated metrics to add.
            additional_info: Additional information to append.

        Raises:
            ModelRegistryError: If update fails.
        """
        try:
            # This is a simplified version - in production, you'd want to
            # fetch the existing card and update it properly
            updates = {}
            if metrics:
                updates["metrics"] = metrics
            if additional_info:
                updates.update(additional_info)

            # For now, we'll just add a comment
            comment = "\n\n## Updates\n\n"
            for key, value in updates.items():
                comment += f"**{key}**: {value}\n\n"

            # Note: This is simplified - you'd want to fetch and properly update the card
            logger.info(f"Model card update prepared for {repo_id}")

        except Exception as e:
            raise ModelRegistryError(f"Failed to update model card: {e}")
